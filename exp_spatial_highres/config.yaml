# config.yaml
selected_model: HOM

models:
  HOM:
    # --- Normalization Constants ---
    normalization:
      div: [
        0.5828150954214903, 
        0.32081668049311035, 
        0.14731300278327022,
        0.11227744424227674, 
        0.06426613855425986
      ]
      sub: [
        0.3891843901814305, 
        -0.029435820951114022, 
        286.4926270135644
      ]
      div_last: [
        6.034802696072576, 
        5.416111672926835, 
        7.628441970823414
      ]

    # --- Model Parameters ---
    odm_params_ilap:
      shape_in: [1, 42, 180, 360]
      embed_dim: 256 
      output_channels: 42
      latent_dim: 512
      num_spatial_layers: 4 
      num_temporal_layers: 3
    
    odm_params_state:
      shape_in: [1, 104, 180, 360]
      embed_dim: 256 
      output_channels: 104
      latent_dim: 512
      num_spatial_layers: 4 
      num_temporal_layers: 8

    # odm_params_state:
    #   shape_in: [1, 55, 180, 360]
    #   embed_dim: 256 
    #   output_channels: 55
    #   latent_dim: 512
    #   num_spatial_layers: 4 
    #   num_temporal_layers: 8
    
    # [Updated] Gating Network Parameters
    gating_params:
      # Flux has both x/y components: channels = 2 * D.
      channels: 24        
      embed_dim: 128      
      latent_dim: 256     
      num_spatial_layers: 4 
      num_temporal_layers: 4 
      mlp_ratio: 4.0
      drop: 0.0
      drop_path: 0.1
    
    hybridom_params:
      res: 0.25
      H: 180
      W: 360
      D: 12
      t_mask_s: 320
      t_mask_e: 400
      R: 6371000
      rho0: 1025.0
      g: 9.81
      Omega: 7.292115e-5
      dt: 7200
      N_step: 12
      ds_factor: 1
      W_s: 40
    
    data_params:
      # Use environment variables to point to your local dataset.
      # If you don't use CMEMS initialization, cmems_path can be any valid NetCDF file path required by your workflow.
      mask_path: './data/mask_025.npy'
      mask_ori_path: './data/mask_025_ori.npy'
      cmems_path: './data/cmems_init_example.nc'


trainings:
  HOM:
    seed: 42
    parallel_method: DistributedDataParallel
    batch_size: 1
    num_workers: 1
    init_lr: 2e-4
    lr_step_size: 10
    lr_gamma: 0.2
    num_steps_train: 30
    num_epochs: 50
    
    # --- Integration Settings ---
    integral_interval: 5          # Steps for training (n)
    integral_interval_test: 60     # Steps for testing
    
    # --- Fine-Tuning Settings ---
    fine_tune: False
    fine_tune_lr: 2e-4
    pre_trained_path_ilap: './checkpoints/ilap_best_model.pth'
    pre_trained_path_state: './checkpoints/hom_best_model.pth'

datas:
  HOM:
    data_path: './data'
    boundary_path: './mask.npy'

loggings:
  HOM:
    backbone: 'hom_cotraining'
    log_dir: ./logs
    checkpoint_dir: ./checkpoints
    result_dir: ./results